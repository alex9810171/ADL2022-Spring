{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022_ADL_HW1_Intent.ipynb","provenance":[],"authorship_tag":"ABX9TyNl4mI7StGqpaA3TCEHDdg1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **2022 ADL HW1**\n","Mount Google Drive first, Then: cd drive/MyDrive/Documents/2022_ADL/HW1"],"metadata":{"id":"Kc9iDwKSS0VG"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"bNX_iw6DASEp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648471087075,"user_tz":-480,"elapsed":14387,"user":{"displayName":"Alex Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-_LAsRtx7Q0vHVneMNkZjptDpiYQG8WbyiXc74A=s64","userId":"18047903736792651990"}},"outputId":"4e0f92ec-3edf-4ee9-87b2-472d2ae9254c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6E07lmrWFa0","executionInfo":{"status":"ok","timestamp":1647758045484,"user_tz":-480,"elapsed":413,"user":{"displayName":"Alex Hsu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-_LAsRtx7Q0vHVneMNkZjptDpiYQG8WbyiXc74A=s64","userId":"18047903736792651990"}},"outputId":"4f6f2759-1c10-471c-b426-063b6d7411c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Mar 20 06:34:03 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["\n","\n","1.   斷詞、用glove mapping+處理沒有對應到的，輸出voca\n","2.   清單項目\n","\n"],"metadata":{"id":"A7gNimf_G6_E"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"15U4E8t4oCV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def collate_batch(batch):\n","    label_list, text_list, offsets = [], [], [0]\n","    for (_label, _text) in batch:\n","         label_list.append(label_pipeline(_label))\n","         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","         text_list.append(processed_text)\n","         offsets.append(processed_text.size(0))\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","    text_list = torch.cat(text_list)\n","    return label_list.to(device), text_list.to(device), offsets.to(device)"],"metadata":{"id":"c61IUvq_Wlgk"},"execution_count":null,"outputs":[]}]}